<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ceph源码分析:BackoffThrottle及问题]]></title>
    <url>%2F2018%2F04%2F28%2Fceph_backoffThrottle_config%2F</url>
    <content type="text"><![CDATA[BackoffThrottle简介J版本进入新的流控方式BackoffThrottle， filesttore和journal都使用这个 源码journal 流控设置1234567891011int FileJournal::set_throttle_params()&#123; stringstream ss; bool valid = throttle.set_params( g_conf-&gt;journal_throttle_low_threshhold, g_conf-&gt;journal_throttle_high_threshhold, g_conf-&gt;filestore_expected_throughput_bytes, g_conf-&gt;journal_throttle_high_multiple, g_conf-&gt;journal_throttle_max_multiple, header.max_size - get_top(), &amp;ss); filestore 流控设置1234567891011121314151617181920int FileStore::set_throttle_params()&#123; stringstream ss; bool valid = throttle_bytes.set_params( g_conf-&gt;filestore_queue_low_threshhold, g_conf-&gt;filestore_queue_high_threshhold, g_conf-&gt;filestore_expected_throughput_bytes, g_conf-&gt;filestore_queue_high_delay_multiple, g_conf-&gt;filestore_queue_max_delay_multiple, g_conf-&gt;filestore_queue_max_bytes, &amp;ss); valid &amp;= throttle_ops.set_params( g_conf-&gt;filestore_queue_low_threshhold, g_conf-&gt;filestore_queue_high_threshhold, g_conf-&gt;filestore_expected_throughput_ops, g_conf-&gt;filestore_queue_high_delay_multiple, g_conf-&gt;filestore_queue_max_delay_multiple, g_conf-&gt;filestore_queue_max_ops, &amp;ss); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 计算延时比率bool BackoffThrottle::set_params( double _low_threshhold, double _high_threshhold, double _expected_throughput, double _high_multiple, double _max_multiple, uint64_t _throttle_max, ostream *errstream)... low_threshhold = _low_threshhold; high_threshhold = _high_threshhold; high_delay_per_count = _high_multiple / _expected_throughput; max_delay_per_count = _max_multiple / _expected_throughput; max = _throttle_max; if (high_threshhold - low_threshhold &gt; 0) &#123; s0 = high_delay_per_count / (high_threshhold - low_threshhold); &#125; else &#123; low_threshhold = high_threshhold; s0 = 0; &#125; if (1 - high_threshhold &gt; 0) &#123; s1 = (max_delay_per_count - high_delay_per_count) / (1 - high_threshhold); &#125; else &#123; high_threshhold = 1; s1 = 0; &#125;// 计算delay时间std::chrono::duration&lt;double&gt; BackoffThrottle::_get_delay(uint64_t c) const&#123; if (max == 0) return std::chrono::duration&lt;double&gt;(0); double r = ((double)current) / ((double)max); if (r &lt; low_threshhold) &#123; return std::chrono::duration&lt;double&gt;(0); &#125; else if (r &lt; high_threshhold) &#123; return c * std::chrono::duration&lt;double&gt;( (r - low_threshhold) * s0); &#125; else &#123; return c * std::chrono::duration&lt;double&gt;( high_delay_per_count + ((r - high_threshhold) * s1)); &#125;&#125; 设计逻辑和使用BackoffThrottle 希望实现一个对每个请求追加一个动态实延，逐步减缓IO，使IO更平滑避免达到阀值后全部则阻塞，表现就是IO实延波动很大filestore_queue_high_delay_multiplefilestore_queue_max_delay_multiple设置为0，也就是比率为0，动态调控是关闭的下面来说一下相关的参数的意义以file的ops为例 filestore_expected_throughput_ops为流控的目标，也就是最终的目标ops filestore_queue_max_ops流控的上限，最大值，整个流控是按照当前水平/最大值的比率来控制的 filestore_queue_high_delay_multiple，filestore_queue_low_threshhold,filestore_queue_high_threshhold有_get_delay函数可以知道，控制分3个等级，第一级别无控制当current/max &lt; filestore_queue_low_threshhold 无控制filestore_queue_low_threshhold &lt; current/max &lt; filestore_queue_high_threshhold, 第二控制阶段filestore_queue_max_delay_multiple 为降速的倍率举个例子 目标是1w， 平均耗时 1000000us/10000 = 100us如果filestore_queue_high_delay_multiple 2filestore_queue_low_threshhold 0.6filestore_queue_high_threshhold 0.9max 5wexpected 1w也就是当io大于3w时,小于4.5w时 每个IO等待0+ms ~ 200us， 随着current的提高逐步到200us filestore_queue_max_delay_multiple 同上filestore_queue_max_delay_multiple = 10则 大于4.5w时， 每个io等待 200us~1000us 问题1 由于get时会加锁，这种模型比较适合单线程模型，一般filestore都是多线程，delay意味着所有线程都有等待，所以时延会成倍增长，如果使用倍率值应该设置的小一些2 这个设计应该是为hdd 等比较慢的介质开发的，ssd的话单filestore这种io模型比较难达到ssd上限3 journal是顺序写，会比filestore快很多无论hdd还是ssd，所以从iops上限制会好很多，但是journal却只有流量限制， 如果使用在journal上加一个ops的限制会使IO平滑许多]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph源码分析:rbd-nbd]]></title>
    <url>%2F2018%2F04%2F16%2Frbd_nbd_analy%2F</url>
    <content type="text"><![CDATA[nbd简介NBD(Network Block Device)让你可以将一个远程主机的磁盘空间,当作一个块设备来使用.就像一块硬盘一样。NBD是内核模块，大部分linux发行版都已经包含它 rbd-nbd通过nbd librbd来映射一个ceph镜像为本地块设备 rbd-nbd使用查看是否安装nbd驱动12&gt;lsmon | grep nbdnbd 20480 1 （如果没有请先下载安装nbd.ko, 一般发行版本都已经默认安装） 安装rbd-nbd如果是自行编译ceph，则选择编译rbd-nbd否侧先查看ceph 版本 12&gt;ceph -vceph version 10.2.7 (50e863e0f4bc8f4b9e31156de690d765af245185) https://download.ceph.com/rpm-jewel/el7/x86_64/ 下载对应版本的rbd-nbd 包 创建镜像12345&gt;./rbd create image3 -s 10M&gt;./rbd listimage1image2image3 映射12&gt;sudo ./rbd-nbd map rbd/image1/dev/nbd0 源码分析源码为src/tools/rbd_nbd/rbd-nbd.cc主要有三个命令map -&gt; do_map NBDServer server(fd[1], image); ioctl(nbd, NBD_DO_IT); server.stop();unmap -&gt; do_unmaplist-mapped -&gt; do_list_mapped_devices 主要的工作流程123456789101112void reader_entry() int r = safe_read_exact(fd, &amp;ctx-&gt;request, sizeof(struct nbd_request)); // 读取命令 ... switch (ctx-&gt;command)&#123; case NBD_CMD_DISC: case NBD_CMD_WRITE: // 如果是写命令会再读取数据 &#125; io_start(pctx); // 放入队列等待操作完成 switch (pctx-&gt;command)&#123; 开始调用librbd执行io操作 &#125;void writer_entry() // 主要是负责发送操作完成的回应 应用场景现在由于考虑到krbd当前还有一些问题作为替换krbd的一个方案 其他nbd的使用参考https://blog.csdn.net/kenera/article/details/16839499https://www.cnblogs.com/doscho/p/7112916.html]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>nbd</tag>
        <tag>rbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph源码分析:ceph 读写流程中的锁 - pg lock]]></title>
    <url>%2F2017%2F10%2F04%2Fceph_lock_pg%2F</url>
    <content type="text"><![CDATA[ceph 读写流程中的锁读写流程的锁主要分两种 pg lock 和对象锁pg lock 主要维护pg的一致性， 所以恢复，写入等操作会加锁 ，通过控制journal这一层来控制一致性（入队，出对要加锁控制版本的变化）ondisk_read_lock / ondisk_write_lock 用来控制同一对象的读写关系，互斥同一对象的读写, 可以参见–(yu) code rgw 11 其中ObjectContextRef用来描述一个对象，操作时会先获取他的锁 rw_locks 处理线程处理线程是osd_op_tp ，这是一个shardedwq这个线程池的几个主要参数osd_op_num_shards 所有的请求取模分到指定的线程组处理osd_op_num_threads_per_shard 每个线程组中线程数osd_op_thread_timeout 线程处理超时阀值osd_op_thread_suicide_timeout 自杀阀值，超过此值osd自动退出1234567891011121314OSD::OSD(...):...1672=&gt;op_shardedwq(1673 cct-&gt;_conf-&gt;osd_op_num_shards,1674 this,1675 cct-&gt;_conf-&gt;osd_op_thread_timeout,1676 cct-&gt;_conf-&gt;osd_op_thread_suicide_timeout,1677 &amp;osd_op_tp),1678 peering_wq(1679 this,1680 cct-&gt;_conf-&gt;osd_op_thread_timeout,1681 cct-&gt;_conf-&gt;osd_op_thread_suicide_timeout,1682 &amp;osd_tp),... 主要处理函数ShardedOpWQ的处理函数可以看到pg 的锁的颗粒度还是比较大的，以pg为单位，所有在高速场景增加pg数量也可以有效提高并发123456789101112131415161718void OSD::ShardedOpWQ::_process(uint32_t thread_index, heartbeat_handle_d *hb )&#123;8697 uint32_t shard_index = thread_index % num_shards; //8714 pair&lt;PGRef, PGQueueable&gt; item = sdata-&gt;pqueue-&gt;dequeue();8717 ThreadPool::TPHandle tp_handle(osd-&gt;cct, hb, timeout_interval,8718 suicide_interval);8720 (item.first)-&gt;lock_suspend_timeout(tp_handle); // 获取pg lock －－－ 主要这三类操作 OP(osd-&gt;dequeue_op=&gt;ReplicatedPG::do_op)，snap trim(pg-&gt;snap_trimmer)，scrub(pg-&gt;scrub)， 见下面8722 boost::optional&lt;PGQueueable&gt; op;8730 op = sdata-&gt;pg_for_processing[&amp;*(item.first)].front();8758 op-&gt;run(osd, item.first, tp_handle); // 根据op类型调用不同处理函数8771 (item.first)-&gt;unlock(); // 释放pg lock&#125; 主要处理三类操作PGSnapTrimPGScrubOpRequestRef同时处理三类操作意味着如果pg 在处理snap trim 或者scrub操作时pg 是无法响应请求的同时如果单个pg的处理时间过长也会导致所属组的线程数量变少，相关的pg处理速度 最差情况是同组所有线程都在处理snap trim 和scrub操作导致映射这组上的其他pg读写操作无法处理，这一点值得注意 所以要缩短scrub操作的处理时间和增大间隔，并在业务空闲时段处理123456789101112131415161718192021347-class PGQueueable &#123;348 -typedef boost::variant&lt;349 OpRequestRef,350 PGSnapTrim,351 PGScrub352 &gt; QVariant;353 QVariant qvariant;354 int cost;355 unsigned priority;356 utime_t start_time;357 entity_inst_t owner;358 -struct RunVis : public boost::static_visitor&lt;&gt; &#123;359 OSD *osd;360 PGRef &amp;pg;361 ThreadPool::TPHandle &amp;handle;362 -RunVis(OSD *osd, PGRef &amp;pg, ThreadPool::TPHandle &amp;handle)363 : osd(osd), pg(pg), handle(handle) &#123;&#125;364 void operator()(OpRequestRef &amp;op);365 void operator()(PGSnapTrim &amp;op);366 void operator()(PGScrub &amp;op);367 &#125;; 相关配置osd_op_num_shardsosd_op_num_threads_per_shardosd_op_thread_timeoutosd_op_thread_suicide_timeout]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>osd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph源码分析:bluestore - StupidAllocator]]></title>
    <url>%2F2017%2F09%2F23%2Fstupidallocator_analy%2F</url>
    <content type="text"><![CDATA[Allocator-StupidAllocator 简介Allocator 主要是用来管理bluestore中空闲空间，处理空闲空间的分配和回收已经分配的空间信息，会以将元数据信息固化在磁盘中，这里bluestore和bluefs不同 bluestore 记录在rocksdb bluefs 是把元数据以日志形式记录在开头主要的管理方式有两种 StupidAllocator 基于区间树的实现 BitMapAllocator 基于bitmapStupidAllocator 分析StupidAllocator 不需要设备信息来初始化，后面添加freelist ， 实现依赖btree_interval_set，此处指分区StupidAllocator 默认有10个btree_interval_set，分配和释放调动allocate和release 分配和归还时会根据归还的空间长度计算bin值来决定操作哪一个区间树， 总体是小空间操作0号，长度越大操作树的编号越大主要函数 allocate : want_size (期望大小) alloc_unit (对其大小单位) max_alloc_size (最大连续空间，大于此空间会返回列表) hint (对地址的要求，最好大于此地址) extents (返回)如果want_size &gt; max_alloc_size 会返回几个地址段可以非；连续alloc_unit 对应 cct-&gt;_conf-&gt;bluefs_alloc_sizeallocate_int want_size(需要空间) alloc_unit(空间单位会和其对齐) hint(空间的下边缘，获得的空间互大于这个地址) offset(返回结果地址) length(返回地址长度)wantsize 和alloc_unit 对齐后计算bin值决定选择哪个区间树（这种策略决定了bin越大的区间树，连续空间越大）1 在&gt;=bin值的free中查找 大于hint的地址区间，看是否足够空闲空间2 在&gt;=bin值的free中查找, 0~hint 的区间是否足够空闲空间3 在&lt;bin值的free中查找 大于hint的地址区间，看是否足够空闲空间4 在&lt;bin值的free中查找, 0~hint 的区间是否足够空闲空间总结：后面的区间会比较倾向于处理大的连续空间放到， 前面的区间倾向处理小额的空间分配和归还，分配和归还都是如此 release归还释放的空间，更新空闲的总空间_insert_free将空间回收后可能会合并成新的区间， 此时需要在把这个区间放到合适bin的freelist中，保证了bin值小的区间连续段也比较小 init_rm_free 根据log 重整空闲空间區間樹，在bluefs重新mount，replay原数据日志时会使用]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>bluestore</tag>
        <tag>allocator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph源码分析:bluestore - bluefs]]></title>
    <url>%2F2017%2F09%2F02%2Fbluefs_analy%2F</url>
    <content type="text"><![CDATA[bluefs 简介bluefs 实现了一个简单的文件系统，只支持追加写，没有实现posix的接口，本身只有一层目录和一层文件的两层结构，为rocksdb 量身定做，符合rocksdb和leveldb的文件结构的数据结构rocksdb的存在在bluestore的架构里用于存储元数据，所以bluefs只能自己 管理元数据具体的方式是开辟专门的空间来记录数据操作日志，恢复时利用日志来构建使用关系，定期压缩日志\ bluefs 结构BlockDevice 用于描述物理设备的读写（hdd ssd 都是KernelDevice）IOContext 用于描述IO方式，此处主要是异步IO（类似libaio）， 需要提交到队列，然后commit，等待返回结果 bluefs 的成员IOContext 主要用于读和super的读（只是作为参数）， super的写是sync写 IOContext 主要用在FileWriter 写时（普通写和操作log写） Allocator 用于描述磁盘的空闲空间管理，已经分配的空间会由具体的存储引擎来负责保存元数据 bluestore 会将元数据保存到rockdb bluefs 将元数据的操作日志保存到磁盘中，重建时需要重新replay日志， 定期compact来减少日志量Allocator 包含两种策略StupidAllocator 和BitMapAllocator StupidAllocator 基于btree_interval_set 的实现 BitMapAllocator 基于bitmap的实现 主要接口及读写流程BlueFS::add_block_device // 添加设备add_block_extent // 添加可用范围mkfs // 首次创建时初始化mount // 如果非首次，挂载时会读取日志重建freelistmkdir // 创建目录，只支持一级目录open_for_write &amp;&amp; BlueFS::FileWriter h // 写文件需要先获得或创建一个写文件句柄h-&gt;append // 写文件fsync BlueFS::FileWriter h // 同步到系统close_writer // 关闭句柄open_for_read // 获得读句柄readumount sync_metadata // 同步原数据 bluefs_compact_log_sync true or falseunlinkflush_logrmdirflush_log // 下刷元数据compact_log// 压缩日志，些时间长了原数据日志很大影响加载时间和空间引用：Inode索引号0和1，由BlueFS内部使用，并且都用于日志文件。正常情况下，日志文件只使用索引号1，只有在Compact日志时才使用临时的索引号0。日志文件只支持Append操作，时间长了，日志文件会占用较大的磁盘空间，并且Replay日志时间也会变久。实际上，日志文件中有用的数据只有，目录文件的元数据以及目录和文件的对应关系。Compact日志的目是，提取这部分有用数据写入到新磁盘空间，并丢掉原来的日志数据，减少日志文件大小，释放磁盘空间。 Compact日志时先使用索引号0的新日志文件将有用数据写入到磁盘的新位置，然后用新日志文件的元数据更新超级块（除索引号外），将超级块Flush到磁盘。从目前实现来看，修改超级块是个危险系数很高的操作，一方面是因为超级块的重要程度，如果超级块数据出错所有文件将都无法使用，另一方面似乎没有对超级块做数据保护，只提供CRC检验。不过，正常情况下极少修改超级块的数据，写磁盘时也会绕过本地缓存 主要数据结构123456789101112131415161718id 设备MAX_BDEV BDEV_WAL BDEV_DB BDEV_SLOW ， 一共三种设备， MAX_BD_EV 对应上限244- /*245 * There are up to 3 block devices:246 *247 * BDEV_DB db/ - the primary db device248 * BDEV_WAL db.wal/ - a small, fast device, specifically for the WAL249 * BDEV_SLOW db.slow/ - a big, slow device, to spill over to as BDEV_DB fills250 */251 vector&lt;BlockDevice*&gt; bdev; ///&lt; block devices we can use // 封装了不同设直接备操作(read write)252 vector&lt;IOContext*&gt; ioc; ///&lt; IOContexts for bdevs // IOContext主要封装读写操作模式（libaio directio）253 vector&lt;interval_set&lt;uint64_t&gt; &gt; block_all; ///&lt; extents in bdev we own254 vector&lt;uint64_t&gt; block_total; ///&lt; sum of block_all255 vector&lt;Allocator*&gt; alloc; ///&lt; allocators for bdevs // 介质空闲空间的管理，已分配的空间落盘时会写入元数据 主要流程源码写流程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647bluefs::open_for_write bluefs 只支持一层目录 dir_map 描述所有目录 file_map 描述所有文件 dir-&gt;file_map 描述目录下文件 File fnode.ino file_map索引 fnode.mtime fnode.prefer_bdev 优先的介质 fnode.extents 空间地址 bluefs_transaction_t log_t 元数据日志，还未写入的 PerfCounters logger只是计数器，用于操作统计 BlueFS::FileWriter *BlueFS::_create_writer(FileRef f) FileWriter::iocv 通过file创建，为每一个介质，创建IOContextbluefs::fsync =&gt;_flush 检测数据长度是否满足bluefs_min_flush_size =&gt;_flush_range(h, offset, length) 计算偏移量，调用相应io方式，direct还是libaio，如果libaio再submit，记录pending io数量 将所有的变动元数据写入dirty_files， 编号写入文件dirty_seq =&gt;_flush_bdev_safely 等待所有aio 完成 =&gt;_flush_and_sync_log(l, old_dirty_seq) 将dirty_seq编号的元数据落盘 =&gt;bluefs_transaction_t log_t.op_file_update(log_writer-&gt;file-&gt;fnode); // 将多有的文件变动写入日志 =&gt;FileWriter* log_writer-&gt;append(bl); // 日志编码成buffer， 记入log_write =&gt;_flush(log_writer, true); // 将内容写入磁盘 =&gt;_flush_bdev_safely(log_writer); // 确保数据落盘 更新统计数据### 初始化流程``` shellBlueFS::add_block_device // 记录使用哪个文件（也可以指定块设备） BlockDevice *b = BlockDevice::create new KernelDevice b.openBlueFS::fs.add_block_extent // 划分设备的使用空间 block_all[id].insert(offset, length) block_total[id] += length; bluefs_transaction_t log_t.op_alloc_add(id, offset, length); // 操作日志 int r = _flush_and_sync_log(l); alloc[id]-&gt;init_add_free(offset, length); BlueFS::mkfs _flush_and_sync_log _pad_bl // 对齐super.block_size，补0]]></content>
      <categories>
        <category>storage</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>bluestore</tag>
        <tag>bluefs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于hexo+github搭建个人blog]]></title>
    <url>%2F2017%2F08%2F11%2Fbuild_blog_with_hexo%2F</url>
    <content type="text"><![CDATA[之前大部分工作笔记记录在Eventnote，由于大部分笔记比较杂乱，之后整理了一部分到CSDN，后来看朋友分享的一些blog，觉得模板式样不错。这里是第一篇文章记录下来自己搭建博客的过程和中间遇到的问题后面可能会单独开一篇整理一下搭建过程中遇到的问题和markdown的语法 搭建博客环境macos okcentos 6.5 okfedore 23 X 安装hexo请参考: https://hexo.io/zh-cn/docs/#安装1234sudo yum install npm.x86_64 // 安装nodejssudo npm install -g hexo-cli // sudo npm install -g hexosudo npm install hexo-deployer-git --save 初始化hexo目录1hexo init . 配置hexo _config.yml 网站主要配置项 source目录 网站的主要内容，里面_post目录下是所有文章的markdown文件 themes目录 hexo的模板文件，里面没一个目录都是一个hexo模板，每个模板有自己的配置文件 package.json 主要是nodejs的依赖，下次构建工作目录时会方便一些 scaffolds目录 存放模板文件 还有一个主要的配置文件在模板目录下 hexo模板模板可以从这里挑一个 https://hexo.io/themes/我选的 https://github.com/iissnan/hexo-theme-next相应的配置说明 https://github.com/iissnan/hexo-theme-next/blob/master/README.cn.md把相应的模板拷贝到themes下默认配置文件是next/_config.yml, 可以参考上面的链接12git clone https://github.com/iissnan/hexo-theme-next themes/nextcd themes/next &amp;&amp; git checkout v5.1.3 &amp;&amp; cd - 测试网站1234hexo ghexo s INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 使用本地浏览器访问http://localhost:4000/ 即可 发布博客关联github账号在github上创建一个和你名字相同的repository : .github.io&gt;, 由于github提供二级域名，所以取其他的名字不行关于github账号的申请，免密码登陆可以参考其他文章，此处不再详细介绍编辑hexo的配置文件123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:&lt;your-github-name&gt;/&lt;your-github-name&gt;.github.io.git branch: master 发布博客到github123hexo chexo ghexo d 此处我参考了一个脚本，作为发布脚本1234567891011121314151617$ cat deploy.sh#!/bin/bashhexo cleanif [[ $? -ne 0 ]]; then echo "hexo clean Error!" return 1fihexo generateif [[ $? -ne 0 ]]; then echo "hexo generate Error!" return 1fihexo deployif [[ $? -ne 0 ]]; then echo "hexo deploy Error!" return 1fi 之后就可以访问你的博客了&lt;your-github-name.github.io&gt; 管理博客源文件再创建一个repository 来管理你的源文件，方便在其他地方搭建博客的的工作目录1234567891011121314151617181920# tree├── _config.yml├── deploy.sh├── scaffolds│ ├── draft.md│ ├── page.md│ └── post.md├── package.json├── db.json├── source│ ├── categories│ │ └── index.md│ ├── _posts│ └── tags│ └── index.md└── themes └── next └── _config.yml7 directories, 11 files 主要管理这些配置文件和源文件就可以如果需要在构建博客的工作目录12345678git clone git@gitee.com:simon29/blog.gitcd blognpm installmv themes/next themes/next_1git clone https://github.com/iissnan/hexo-theme-next themes/nextcd themes/next &amp;&amp; git checkout v5.1.3 &amp;&amp; cd -cp themes/next_1/_config.yml themes/nextrm -rf themes/next_1 申请域名与域名绑定这个我还没弄;) blog优化及遇到的问题 Hexo之next主题设置首页不显示全文_config.yml开启auto_excerpt项或者用md写的话，在文章里加 会自动截断refhttps://www.jianshu.com/p/393d067dba8d 设置语言在hexo 文件夹里的_config.yml 修改 language: zh-CN 显示网站统计themes/next/_config.yml开启 busuanzi_count编辑器推荐typora 小巧，跨平台，所见即所得官网 https://www.typora.io/参考链接https://www.cnblogs.com/MuYunyun/p/5927491.htmlhttp://www.yangguanjun.com/2017/05/21/hexo-pagesgithub-build-website/https://github.com/chriskempson/tomorrow-themehttps://www.jianshu.com/p/4ef35521fee9http://blog.csdn.net/superchao_5/article/details/51963942http://blog.csdn.net/wx_962464/article/details/44786929]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
